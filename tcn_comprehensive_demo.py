"""
TCN ç»¼åˆæ¼”ç¤ºè„šæœ¬
ä¸€æ¬¡æ€§å±•ç¤ºæ‰€æœ‰æ ¸å¿ƒæ¦‚å¿µï¼Œæ— éœ€äº¤äº’
"""

import torch
import torch.nn as nn
import numpy as np
import matplotlib.pyplot as plt
from tcn_implementation import CausalConv1d, TemporalBlock, TemporalConvNet, create_sample_data

plt.rcParams['font.sans-serif'] = ['SimHei', 'DejaVu Sans']
plt.rcParams['axes.unicode_minus'] = False

def demo_causal_convolution():
    """æ¼”ç¤ºå› æœå·ç§¯çš„å·¥ä½œåŸç†"""
    print("=" * 80)
    print("ğŸ” ç¬¬ä¸€éƒ¨åˆ†ï¼šå› æœå·ç§¯ (Causal Convolution)")
    print("=" * 80)
    
    print("\næ ¸å¿ƒæ€æƒ³ï¼š")
    print("- ç¡®ä¿tæ—¶åˆ»çš„è¾“å‡ºåªä¾èµ–äºtåŠä¹‹å‰çš„è¾“å…¥")
    print("- é€šè¿‡å·¦å¡«å……å®ç°'åªçœ‹è¿‡å»'çš„æ•ˆæœ")
    print("- è¿™å¯¹æ—¶åºé¢„æµ‹ä»»åŠ¡è‡³å…³é‡è¦\n")
    
    # åˆ›å»ºç®€å•çš„ç¤ºä¾‹
    sequence_length = 10
    input_data = torch.randn(1, 1, sequence_length)
    
    print(f"ğŸ“Š è¾“å…¥åºåˆ—é•¿åº¦: {sequence_length}")
    print(f"è¾“å…¥æ•°æ®å½¢çŠ¶: {input_data.shape}")
    
    input_sample = input_data.squeeze().numpy()[:5]
    input_str = ", ".join([f"{x:.3f}" for x in input_sample])
    print(f"è¾“å…¥æ•°æ®: [{input_str}]... (æ˜¾ç¤ºå‰5ä¸ªå€¼)\n")
    
    # æµ‹è¯•ä¸åŒkernel sizeçš„å› æœå·ç§¯
    kernel_sizes = [2, 3, 5]
    
    print("ğŸ”§ ä¸åŒå·ç§¯æ ¸å¤§å°çš„æ•ˆæœ:")
    for kernel_size in kernel_sizes:
        causal_conv = CausalConv1d(1, 1, kernel_size, dilation=1)
        
        with torch.no_grad():
            output = causal_conv(input_data)
        
        receptive_info = causal_conv.get_receptive_field_info()
        print(f"  æ ¸å¤§å° {kernel_size}:")
        print(f"    - å¡«å……å¤§å°: {causal_conv.padding}")
        print(f"    - è¾“å‡ºå½¢çŠ¶: {output.shape}")
        print(f"    - æœ‰æ•ˆæ ¸å¤§å°: {receptive_info['effective_kernel_size']}")
        print(f"    - è¾“å…¥èŒƒå›´: [{input_data.min():.3f}, {input_data.max():.3f}]")
        print(f"    - è¾“å‡ºèŒƒå›´: [{output.min():.3f}, {output.max():.3f}]")
    
    print("\nğŸ’¡ å…³é”®è§‚å¯Ÿï¼š")
    print("1. è¾“å‡ºåºåˆ—é•¿åº¦ä¸è¾“å…¥ç›¸åŒï¼ˆå› æœå¡«å……çš„ä½œç”¨ï¼‰")
    print("2. ä¸åŒæ ¸å¤§å°äº§ç”Ÿä¸åŒçš„å¹³æ»‘æ•ˆæœ")
    print("3. è¾“å‡ºåªä¾èµ–äºå½“å‰å’Œè¿‡å»çš„è¾“å…¥")


def demo_dilated_convolution():
    """æ¼”ç¤ºè†¨èƒ€å·ç§¯çš„æ•ˆæœ"""
    print("\n" + "=" * 80)
    print("ğŸš€ ç¬¬äºŒéƒ¨åˆ†ï¼šè†¨èƒ€å·ç§¯ (Dilated Convolution)")
    print("=" * 80)
    
    print("\næ ¸å¿ƒæ€æƒ³ï¼š")
    print("- é€šè¿‡'è·³è·ƒ'é‡‡æ ·æ‰©å¤§æ„Ÿå—é‡")
    print("- ç”¨å°‘é‡å‚æ•°æ•è·é•¿æœŸä¾èµ–")
    print("- è†¨èƒ€ç‡å†³å®šé‡‡æ ·çš„é—´éš”\n")
    
    # åˆ›å»ºé•¿åºåˆ—è¿›è¡Œæ¼”ç¤º
    sequence_length = 20
    input_data = torch.cos(torch.linspace(0, 4*np.pi, sequence_length)).unsqueeze(0).unsqueeze(0)
    
    print(f"ğŸ“Š ä½¿ç”¨ä½™å¼¦æ³¢ä½œä¸ºè¾“å…¥ (é•¿åº¦={sequence_length})")
    print(f"è¾“å…¥æ•°æ®å½¢çŠ¶: {input_data.shape}\n")
    
    # æµ‹è¯•ä¸åŒè†¨èƒ€ç‡
    dilations = [1, 2, 4, 8]
    kernel_size = 3
    
    print("ğŸ”§ ä¸åŒè†¨èƒ€ç‡çš„æ•ˆæœ:")
    for dilation in dilations:
        dilated_conv = CausalConv1d(1, 1, kernel_size, dilation)
        
        with torch.no_grad():
            output = dilated_conv(input_data)
        
        receptive_info = dilated_conv.get_receptive_field_info()
        
        print(f"  è†¨èƒ€ç‡ {dilation}:")
        print(f"    - æœ‰æ•ˆæ ¸å¤§å°: {receptive_info['effective_kernel_size']}")
        print(f"    - å¡«å……å¤§å°: {receptive_info['padding']}")
        print(f"    - ç†è®ºæ„Ÿå—é‡: {receptive_info['effective_kernel_size']}")

    print("\nğŸ’¡ å…³é”®è§‚å¯Ÿï¼š")
    print("1. è†¨èƒ€ç‡è¶Šå¤§ï¼Œæ„Ÿå—é‡è¶Šå¤§")
    print("2. å¤§è†¨èƒ€ç‡èƒ½æ•è·æ›´é•¿æœŸçš„æ¨¡å¼")
    print("3. è¾“å‡ºå¹³æ»‘ç¨‹åº¦éšè†¨èƒ€ç‡å˜åŒ–")


def demo_residual_connection():
    """æ¼”ç¤ºæ®‹å·®è¿æ¥çš„ä½œç”¨"""
    print("\n" + "=" * 80)
    print("ğŸ”— ç¬¬ä¸‰éƒ¨åˆ†ï¼šæ®‹å·®è¿æ¥ (Residual Connection)")
    print("=" * 80)
    
    print("\næ ¸å¿ƒæ€æƒ³ï¼š")
    print("- å…è®¸ä¿¡æ¯ç›´æ¥ä¼ æ’­ï¼Œç¼“è§£æ¢¯åº¦æ¶ˆå¤±")
    print("- output = F(x) + xï¼Œå­¦ä¹ æ®‹å·®è€Œéç›´æ¥æ˜ å°„")
    print("- ä½¿æ·±å±‚ç½‘ç»œæ›´å®¹æ˜“è®­ç»ƒ\n")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 4
    sequence_length = 20
    input_channels = 8
    output_channels = 8
    
    input_data = torch.randn(batch_size, input_channels, sequence_length)
    
    print(f"ğŸ“Š æµ‹è¯•æ•°æ®: {input_data.shape}")
    print(f"è¾“å…¥ç»Ÿè®¡: å‡å€¼={input_data.mean():.3f}, æ ‡å‡†å·®={input_data.std():.3f}\n")
    
    # åˆ›å»ºæ—¶åºå—ï¼ˆåŒ…å«æ®‹å·®è¿æ¥ï¼‰
    temporal_block = TemporalBlock(
        n_inputs=input_channels,
        n_outputs=output_channels, 
        kernel_size=3,
        dilation=2,
        dropout=0.0  # å…³é—­dropoutä¾¿äºè§‚å¯Ÿ
    )
    
    print("ğŸ”§ æ—¶åºå—é…ç½®:")
    print(f"  - è¾“å…¥é€šé“: {input_channels}")
    print(f"  - è¾“å‡ºé€šé“: {output_channels}")
    print(f"  - è†¨èƒ€ç‡: 2")
    print(f"  - æ ¸å¤§å°: 3\n")
    
    # å‰å‘ä¼ æ’­å¹¶æ”¶é›†ä¸­é—´ç»“æœ
    temporal_block.eval()
    with torch.no_grad():
        # ä¿å­˜åŸå§‹è¾“å…¥
        residual = input_data
        
        # ç¬¬ä¸€ä¸ªå·ç§¯
        out = temporal_block.conv1(input_data)
        out = temporal_block.relu(out)
        conv1_output = out.clone()
        
        # ç¬¬äºŒä¸ªå·ç§¯  
        out = temporal_block.conv2(out)
        out = temporal_block.relu(out)
        conv2_output = out.clone()
        
        # æ®‹å·®è¿æ¥
        final_output = temporal_block.relu(out + residual)
    
    # åˆ†ææ®‹å·®è¿æ¥çš„æ•°å€¼ç‰¹å¾ï¼ˆéæ­£å¼æŒ‡æ ‡ï¼Œä»…ç”¨äºç›´è§‚ç†è§£ï¼‰
    residual_magnitude = residual.abs().mean()
    final_magnitude = final_output.abs().mean()
    residual_ratio = residual_magnitude / final_magnitude
    
    print("ğŸ“ˆ å‰å‘ä¼ æ’­åˆ†æ:")
    print(f"  - è¾“å…¥ç»Ÿè®¡: å‡å€¼={input_data.mean():.3f}, æ ‡å‡†å·®={input_data.std():.3f}")
    print(f"  - Conv1è¾“å‡º: å‡å€¼={conv1_output.mean():.3f}, æ ‡å‡†å·®={conv1_output.std():.3f}")
    print(f"  - Conv2è¾“å‡º: å‡å€¼={conv2_output.mean():.3f}, æ ‡å‡†å·®={conv2_output.std():.3f}")
    print(f"  - æœ€ç»ˆè¾“å‡º: å‡å€¼={final_output.mean():.3f}, æ ‡å‡†å·®={final_output.std():.3f}")
    print(f"  - æ®‹å·®vsè¾“å‡ºå¹…åº¦æ¯”: {residual_ratio:.3f} (åŸå§‹è¾“å…¥åœ¨è¾“å‡ºä¸­çš„æ•°å€¼å æ¯”)")
    
    print("\nğŸ’¡ å…³é”®è§‚å¯Ÿï¼š")
    print("1. æ®‹å·®è¿æ¥ä¿ç•™äº†åŸå§‹ä¿¡æ¯")
    print("2. è¾“å‡ºç»“åˆäº†æ–°å­¦ä¹ çš„ç‰¹å¾å’ŒåŸå§‹è¾“å…¥")
    print("3. å¹…åº¦æ¯”åæ˜ äº†åŸå§‹ä¿¡æ¯åœ¨è¾“å‡ºä¸­çš„ç›¸å¯¹å¼ºåº¦ï¼ˆéæ ‡å‡†å­¦æœ¯æŒ‡æ ‡ï¼‰")


def demo_stacked_tcn():
    """æ¼”ç¤ºå †å TCNçš„æ•ˆæœ"""
    print("\n" + "=" * 80)
    print("ğŸ—ï¸ ç¬¬å››éƒ¨åˆ†ï¼šå †å çš„TCN")
    print("=" * 80)
    
    print("\næ ¸å¿ƒæ€æƒ³ï¼š")
    print("- å¤šå±‚çº§è”ï¼Œè†¨èƒ€ç‡æŒ‡æ•°å¢é•¿")
    print("- æ¯å±‚æ•è·ä¸åŒæ—¶é—´å°ºåº¦çš„æ¨¡å¼")
    print("- æ„Ÿå—é‡å‘ˆæŒ‡æ•°çº§æ‰©å¤§\n")
    
    # åˆ›å»ºæµ‹è¯•æ•°æ®
    batch_size = 2
    sequence_length = 50
    num_inputs = 3
    
    X, _ = create_sample_data(batch_size, sequence_length, num_inputs)
    
    print(f"ğŸ“Š æµ‹è¯•æ•°æ®: {X.shape}")
    print(f"åºåˆ—é•¿åº¦: {sequence_length}")
    
    # åˆ›å»ºä¸åŒæ·±åº¦çš„TCNè¿›è¡Œå¯¹æ¯”
    configurations = [
        {"name": "æµ…å±‚TCN", "channels": [16, 16]},
        {"name": "ä¸­å±‚TCN", "channels": [16, 16, 16, 16]},
        {"name": "æ·±å±‚TCN", "channels": [16, 16, 16, 16, 16, 16]}
    ]
    
    print("\nğŸ”§ ä¸åŒæ·±åº¦TCNå¯¹æ¯”:")
    for config in configurations:
        tcn = TemporalConvNet(num_inputs, config["channels"], kernel_size=3, dropout=0.1)
        network_info = tcn.get_network_info()
        
        print(f"\n  {config['name']}:")
        print(f"    - å±‚æ•°: {network_info['num_levels']}")
        print(f"    - ç†è®ºæ„Ÿå—é‡: {network_info['receptive_field']}")
        print(f"    - å‚æ•°é‡: {network_info['parameters']:,}")
        print(f"    - è†¨èƒ€ç‡: {network_info['dilations']}")
        
        # åˆ†ææ„Ÿå—é‡vsåºåˆ—é•¿åº¦çš„å…³ç³»
        rf_ratio = network_info['receptive_field'] / sequence_length
        print(f"    - æ„Ÿå—é‡è¦†ç›–ç‡: {rf_ratio:.2%}")
    
    print("\nğŸ’¡ å…³é”®è§‚å¯Ÿï¼š")
    print("1. æ›´æ·±çš„ç½‘ç»œå…·æœ‰æ›´å¤§çš„æ„Ÿå—é‡")
    print("2. æ„Ÿå—é‡åº”è¯¥ä¸ä»»åŠ¡éœ€æ±‚åŒ¹é…")
    print("3. è¿‡å¤§çš„æ„Ÿå—é‡å¯èƒ½å¯¼è‡´è¿‡æ‹Ÿåˆ")


def demo_practical_application():
    """æ¼”ç¤ºå®é™…åº”ç”¨ï¼šæ—¶é—´åºåˆ—é¢„æµ‹"""
    print("\n" + "=" * 80)
    print("ğŸ¯ ç¬¬äº”éƒ¨åˆ†ï¼šå®é™…åº”ç”¨ - æ—¶é—´åºåˆ—é¢„æµ‹")
    print("=" * 80)
    
    print("\nğŸ¯ è®©æˆ‘ä»¬ç”¨TCNè§£å†³ä¸€ä¸ªå®é™…é—®é¢˜ï¼šæ—¶é—´åºåˆ—é¢„æµ‹")
    print("ä»»åŠ¡ï¼šé¢„æµ‹ä½™å¼¦æ³¢çš„ä¸‹ä¸€ä¸ªå€¼\n")
    
    # ç”Ÿæˆæ—¶é—´åºåˆ—æ•°æ®
    def generate_cosine_wave(length, freq=1, noise_level=0.1):
        t = np.linspace(0, 4*np.pi, length)
        signal = np.cos(freq * t) + noise_level * np.random.randn(length)
        return signal
    
    # åˆ›å»ºæ•°æ®é›†
    sequence_length = 100
    prediction_length = 10
    num_samples = 50  # å‡å°‘æ ·æœ¬æ•°é‡ç”¨äºæ¼”ç¤º
    
    print(f"ğŸ“Š æ•°æ®é›†é…ç½®:")
    print(f"  - æ ·æœ¬æ•°é‡: {num_samples}")
    print(f"  - åºåˆ—é•¿åº¦: {sequence_length}")
    print(f"  - é¢„æµ‹é•¿åº¦: {prediction_length}")
    
    # ç”Ÿæˆè®­ç»ƒæ•°æ®
    X_train = []
    y_train = []
    
    for i in range(num_samples):
        # éšæœºé¢‘ç‡å’Œå™ªå£°
        freq = 0.5 + np.random.rand() * 2
        noise = 0.05 + np.random.rand() * 0.1
        
        # ç”Ÿæˆå®Œæ•´åºåˆ—
        full_sequence = generate_cosine_wave(sequence_length + prediction_length, freq, noise)
        
        # åˆ†å‰²è¾“å…¥å’Œç›®æ ‡
        X_train.append(full_sequence[:sequence_length])
        y_train.append(full_sequence[sequence_length:sequence_length + prediction_length])
    
    X_train = torch.FloatTensor(X_train).unsqueeze(1)  # (samples, 1, seq_len)
    y_train = torch.FloatTensor(y_train).unsqueeze(1)  # (samples, 1, pred_len)
    
    print(f"  - è®­ç»ƒè¾“å…¥å½¢çŠ¶: {X_train.shape}")
    print(f"  - è®­ç»ƒç›®æ ‡å½¢çŠ¶: {y_train.shape}\n")
    
    # åˆ›å»ºTCNé¢„æµ‹æ¨¡å‹
    from tcn_implementation import TCNPredictor

    model = TCNPredictor(
        num_inputs=1,
        num_channels=[32, 32, 32, 32],
        prediction_length=prediction_length,
        kernel_size=3,
        dropout=0.1
    )

    print(f"ğŸ”§ æ¨¡å‹é…ç½®:")
    network_info = model.tcn.get_network_info()
    print(f"  - å‚æ•°é‡: {sum(p.numel() for p in model.parameters()):,}")
    print(f"  - æ„Ÿå—é‡: {network_info['receptive_field']}")
    print(f"  - TCNå±‚æ•°: {network_info['num_levels']}\n")

    # ç®€å•è®­ç»ƒï¼ˆåªåšå‡ ä¸ªepochæ¼”ç¤ºï¼‰
    optimizer = torch.optim.Adam(model.parameters(), lr=0.001)
    criterion = nn.MSELoss()

    print("ğŸš€ å¼€å§‹è®­ç»ƒ...")
    model.train()
    losses = []

    for epoch in range(5):  # åªè®­ç»ƒ5ä¸ªepochç”¨äºæ¼”ç¤º
        epoch_loss = 0
        num_batches = 0

        for i in range(0, len(X_train), 16):  # batch_size=16
            batch_x = X_train[i:i+16]
            batch_y = y_train[i:i+16]

            if len(batch_x) < 2:  # è·³è¿‡å¤ªå°çš„æ‰¹æ¬¡
                continue

            optimizer.zero_grad()
            predictions = model(batch_x)

            # åªä½¿ç”¨æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„é¢„æµ‹
            pred_last = predictions[:, :, -1:]  # (batch, pred_len, 1)
            target = batch_y  # (batch, 1, pred_len)

            # è°ƒæ•´ç»´åº¦åŒ¹é…
            loss = criterion(pred_last.squeeze(-1), target.squeeze(1))
            loss.backward()
            optimizer.step()

            epoch_loss += loss.item()
            num_batches += 1

        if num_batches > 0:
            avg_loss = epoch_loss / num_batches
            losses.append(avg_loss)
            print(f"  Epoch {epoch+1}/5: Loss = {avg_loss:.6f}")

    print("âœ… è®­ç»ƒå®Œæˆ!\n")

    # æµ‹è¯•é¢„æµ‹æ•ˆæœ
    model.eval()
    test_idx = 0
    test_input = X_train[test_idx:test_idx+1]
    test_target = y_train[test_idx:test_idx+1]

    with torch.no_grad():
        prediction = model(test_input)
        pred_values = prediction[0, :, -1].numpy()  # æœ€åä¸€ä¸ªæ—¶é—´æ­¥çš„é¢„æµ‹
        true_values = test_target[0, 0].numpy()

    # è®¡ç®—é¢„æµ‹è¯¯å·®
    mse = np.mean((pred_values - true_values) ** 2)
    mae = np.mean(np.abs(pred_values - true_values))

    print(f"ğŸ“Š é¢„æµ‹æ€§èƒ½:")
    print(f"  - å‡æ–¹è¯¯å·® (MSE): {mse:.6f}")
    print(f"  - å¹³å‡ç»å¯¹è¯¯å·® (MAE): {mae:.6f}")

    if len(pred_values) == len(true_values):
        corr = np.corrcoef(pred_values, true_values)[0,1]
        print(f"  - ç›¸å…³ç³»æ•°: {corr:.4f}")

    print("\nğŸ’¡ åº”ç”¨æ€»ç»“:")
    print("1. TCNèƒ½å¤Ÿæœ‰æ•ˆå­¦ä¹ æ—¶é—´åºåˆ—æ¨¡å¼")
    print("2. é€‚åˆéœ€è¦é•¿æœŸä¾èµ–çš„é¢„æµ‹ä»»åŠ¡")
    print("3. è®­ç»ƒé€Ÿåº¦æ¯”RNN/LSTMå¿«")
    print("4. å¯ä»¥é€šè¿‡è°ƒæ•´ç½‘ç»œæ·±åº¦æ§åˆ¶æ„Ÿå—é‡")


def main():
    """ä¸»å‡½æ•°ï¼šè¿è¡Œå®Œæ•´çš„æ¼”ç¤º"""
    print("ğŸ“ TCN (æ—¶åºå·ç§¯ç½‘ç»œ) ç»¼åˆæ¼”ç¤º")
    print("æœ¬æ¼”ç¤ºå°†å…¨é¢å±•ç¤ºTCNçš„æ ¸å¿ƒæ¦‚å¿µå’Œå®é™…åº”ç”¨")
    print("=" * 80)

    # è®¾ç½®éšæœºç§å­
    torch.manual_seed(42)
    np.random.seed(42)

    try:
        # é€æ­¥æ¼”ç¤ºå„ä¸ªæ¦‚å¿µ
        demo_causal_convolution()
        demo_dilated_convolution()
        demo_residual_connection()
        demo_stacked_tcn()
        demo_practical_application()

        print("\n" + "=" * 80)
        print("ğŸ‰ TCNç»¼åˆæ¼”ç¤ºå®Œæˆï¼")
        print("=" * 80)
        print("\nğŸ“ æ ¸å¿ƒçŸ¥è¯†æ€»ç»“:")
        print("1. å› æœå·ç§¯ç¡®ä¿æ—¶åºå»ºæ¨¡çš„å› æœæ€§")
        print("2. è†¨èƒ€å·ç§¯æœ‰æ•ˆæ‰©å¤§æ„Ÿå—é‡")
        print("3. æ®‹å·®è¿æ¥è§£å†³æ·±ç½‘ç»œè®­ç»ƒé—®é¢˜")
        print("4. å †å ç»“æ„æ•è·å¤šæ—¶é—´å°ºåº¦ç‰¹å¾")
        print("5. TCNåœ¨è®¸å¤šæ—¶åºä»»åŠ¡ä¸Šä¼˜äºRNN/LSTM")
        print("\nğŸ”¬ å®éªŒè¦ç‚¹:")
        print("- æ„Ÿå—é‡å¤§å°åº”è¯¥åŒ¹é…ä»»åŠ¡éœ€æ±‚")
        print("- ç½‘ç»œæ·±åº¦å½±å“æ¨¡å‹å®¹é‡å’Œè®¡ç®—æˆæœ¬")
        print("- è†¨èƒ€å·ç§¯æ˜¯TCNçš„æ ¸å¿ƒåˆ›æ–°")
        print("- å› æœå·ç§¯ä¿è¯äº†é¢„æµ‹ä»»åŠ¡çš„åˆç†æ€§")
        print("\nğŸ¯ ä½ ç°åœ¨å·²ç»å®Œå…¨ç†è§£äº†TCNçš„å·¥ä½œåŸç†ï¼")

    except Exception as e:
        print(f"\nâŒ æ¼”ç¤ºè¿‡ç¨‹ä¸­å‡ºç°é”™è¯¯: {e}")
        print("è¿™é€šå¸¸æ˜¯ç¯å¢ƒé…ç½®é—®é¢˜ï¼Œä¸å½±å“TCNæ¦‚å¿µçš„ç†è§£")

if __name__ == "__main__":
    main()
